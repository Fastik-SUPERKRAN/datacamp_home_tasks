{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "# Deep learning\n",
    "\n",
    "</font>\n",
    "\n",
    "\n",
    "<img src = \"data/19_1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<font color = green >\n",
    "\n",
    "## Notation\n",
    "\n",
    "</font>\n",
    "\n",
    "$n_{x}$ - number of features <br>\n",
    "$m$ - number of samples<br>\n",
    "$X$ - input features of shape = $(n_{x}, m)$ <br> \n",
    "$Y$ - labels of shape = $(1, m)$ <br> \n",
    "$L$ - number of layers (excluding input layer)<br> \n",
    "Index $[l]$ corresponds to layer number $l \\in (1...L)$ <br> \n",
    "Index $(i)$ corresponds to sample number $i \\in (1,m) $<br> \n",
    "Bottom index corresponds to unit number e.g. $a^{[2](3)}_{4}$ is the 4th activation unit in 2nd layer of 3rd sample<br> \n",
    "$n^{[l]}$  - number of units in layer $l$<br> \n",
    "$g^{[l]}$ - activation function of layer $l$ <br>\n",
    "$A^{[l]} = g^{[l]}(Z^{[l]})$  - pos-activation values of layer $l \\in (1...L)$  ($A^{[0]} = X$ ) <br>\n",
    "$W^{[l]}$ and $b^{[l]} $  - weights and bias of layer $l$\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "    \n",
    "## Shapes\n",
    "\n",
    "</font>\n",
    "\n",
    "#### Sample for 4 layer neural network \n",
    "\n",
    "\n",
    "<img src = \"data/19_2.png\" align = 'left'>\n",
    "<br>$\n",
    "\\quad n^{[0]} = n_{x} =  2\\\\\n",
    "\\quad n^{[1]} = 3\\\\\n",
    "\\quad n^{[2]} = 5\\\\\n",
    "\\quad n^{[3]} = 4\\\\\n",
    "\\quad n^{[4]} = 2\\\\\n",
    "\\quad n^{[5\n",
    "]} = 1$\n",
    "\n",
    "<div style=\"clear:left;\"></div>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Shape of $\\quad W^{[1]} = (n^{[1]}, n^{[0]}) = (3,2)\\quad\\quad\\quad$     Shape of $\\quad b^{[1]} = (n^{[1]}, 1) = (3,1)$ <br>\n",
    "Shape of $\\quad W^{[2]} = (n^{[2]}, n^{[1]})= (5,3)\\quad\\quad\\quad$     Shape of $\\quad b^{[2]} = (n^{[2]}, 1) = (5,1)$ <br>\n",
    "Shape of $\\quad W^{[3]} = (n^{[3]}, n^{[2]})= (4,5)\\quad\\quad\\quad$     Shape of $\\quad b^{[3]} = (n^{[3]}, 1) = (4,1)$ <br>\n",
    "Shape of $\\quad W^{[4]} = (n^{[4]}, n^{[3]})= (2,4)\\quad\\quad\\quad$     Shape of $\\quad b^{[4]} = (n^{[4]}, 1) = (2,1)$ <br>\n",
    "Shape of $\\quad W^{[5]} = (n^{[5]}, n^{[4]})= (1,2)\\quad\\quad\\quad$     Shape of $\\quad b^{[5]} = (n^{[5]}, 1) = (1,1)$ <br>\n",
    "<br>In general:\n",
    "<br>Shape of $\\quad W^{[l]} = (n^{[l]}, n^{[l-1]})\\,\\quad$     Shape of $\\quad b^{[l]} = (n^{[l]}, 1)$ \n",
    "\n",
    "<br>Shape of $$\\quad A^{[l]}, Z^{[l]}, \\frac{d\\mathcal{L}}{\\partial a^{[l]}}, \\frac{\\partial\\mathcal{L}}{\\partial z^{[l]}} = (n^{[l]}, m) \\quad,\\quad  \\frac{\\partial \\mathcal{L}}{\\partial b^{[l]}} = (n^{[l]}, 1)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "    \n",
    "## Forward propagation \n",
    "\n",
    "</font>\n",
    "\n",
    "Whole process: \n",
    "\\[LINEAR -> RELU\\] $\\times$ (L-1) -> \\[LINEAR -> SIGMOID\\]\n",
    "\n",
    "\n",
    "$A^{[0]} = X$ - input layer \n",
    "\n",
    "Iterate for all $l \\in (1.. L)$:\n",
    "\n",
    "$\\quad\\quad\\quad A^{[l]} = g(Z^{[l]}) = g(W^{[l]}A^{[l-1]} +b^{[l]})$, where $g(Z)$ is one of activation functions: \n",
    "\n",
    "$$\\sigma(Z) = \\frac{1}{ 1 + e^{-Z}}, \\quad\\quad\\quad RELU(Z) = max(0, Z)$$\n",
    "\n",
    "$\\hat{Y} = A^{[L]}$ - output layer (predicted value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "    \n",
    "## Cost Function\n",
    "\n",
    "\n",
    "</font>\n",
    "\n",
    "$$ \\mathcal{L} = -\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right))$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "    \n",
    "## Backward propagation\n",
    "\n",
    "</font>\n",
    "\n",
    "Compute the derivative for last layer: \n",
    "$$\\frac{\\partial \\mathcal{L}}{dA^{[L]}} =  -\\frac{Y}{A^{[L]}} + \\frac{1 - Y}{1 - A^{[L]}}$$\n",
    "\n",
    "Iterate through all layres back:\n",
    "\n",
    "$$ \\frac{\\partial \\mathcal{L} }{\\partial Z^{[l]}} = \\frac{\\partial \\mathcal{L} }{\\partial A^{[l]}} \\cdot \n",
    "\\frac{\\partial g^{[L]}}{\\partial z}(Z^{[L]}); \\quad \n",
    "\\sigma\\,(z)=\\frac {1}{1+{e}^{-z}} \\Rightarrow \n",
    "\\frac { d\\sigma }{ dz } = \\sigma(z)(1-\\sigma(z));\n",
    "\\quad  \n",
    "RELU(z) = max(0, z)\\Rightarrow \n",
    "\\frac { d}{dz}(RELU) = \\begin{cases} 0, z \\le 0 \\\\ 1,\\quad z > 0\\quad \\end{cases}\n",
    "\\\\ \\quad \\\\\n",
    "\\frac{\\partial \\mathcal{L} }{\\partial W^{[l]}} = \\frac{1}{m} \\frac{\\partial \\mathcal{L} }{\\partial Z^{[l]}} \\,@\\, A^{[l-1] T} \\quad \\quad  \n",
    "\\frac{\\partial \\mathcal{L} }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} \\frac{\\partial \\mathcal{L} }{\\partial Z^{[l]}} \\quad (axis=1)\\quad \\quad  \n",
    "\\frac{\\partial \\mathcal{L} }{\\partial A^{[l-1]}} = W^{[l] T} \\,@\\, \\frac{\\partial \\mathcal{L} }{\\partial Z^{[l]}}$$\n",
    "\n",
    "<br>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "    \n",
    "## Forward and backward propagation diagram\n",
    "\n",
    "</font>\n",
    "\n",
    "\n",
    "<img src = \"data/19_forward_backward.png\" align = 'left'>\n",
    "\n",
    "\n",
    "$$\\quad$$\n",
    "\n",
    "Note: It may be worth to cashe the $b^{[l]}$ parameters with other $ W^{[l]} ,A^{[l-1]}, Z^{[l]}$ to make sure the shape of $\\frac {\\partial\\mathcal{L}}{\\partial b^{[l]}}$ is the same as shape of $b^{[l]}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "    \n",
    "## Garadient descent \n",
    "\n",
    "</font>\n",
    "\n",
    "For  $l \\in (1.. L)$:\n",
    "\n",
    "$$W^{[l]}  = W^{[l]} - \\alpha \\cdot \\frac{\\partial \\mathcal{L}}{\\partial W^{[l]}} \\quad\\quad\n",
    "b^{[l]}  = b^{[l]} - \\alpha \\cdot \\frac{\\partial \\mathcal{L}}{\\partial b^{[l]}}$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py \n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import ndimage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "    \n",
    "## Implementation steps \n",
    "\n",
    "</font>\n",
    "\n",
    " - Prepare data \n",
    " - Initialize parameters \n",
    " - Implement forward propagation step \n",
    " - Сompute сost\n",
    " - Init backward propagation\n",
    " - Implement backward propagation step\n",
    " - Update parameters in gradient descent \n",
    " - (Build two-layer model)\n",
    " - Train model \n",
    " - Evaluate model\n",
    " - Implement forward propagation whole process \n",
    " - Implement backward propagation whole process \n",
    " - Build deep neural network model \n",
    " - Predict \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "## Prepare data\n",
    "\n",
    "</font>\n",
    "\n",
    "Cat vs Non-cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### Load data\n",
    "\n",
    "</font>\n",
    "\n",
    "Cat vs Non-cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "cwd= os.getcwd() # current working directory\n",
    "path = os.path.join(cwd,'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    file_name=  os.path.join(path , 'train_catvnoncat.h5')\n",
    "    train_dataset = h5py.File(file_name, \"r\")\n",
    "    X_train = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n",
    "    Y_train = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n",
    "\n",
    "    file_name=  os.path.join(path , 'test_catvnoncat.h5')\n",
    "    test_dataset = h5py.File(file_name, \"r\")\n",
    "    X_test = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n",
    "    Y_test = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n",
    "\n",
    "    classes = ['non-cat','cat']\n",
    "\n",
    "    return X_train, Y_train, X_test, Y_test, classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "###  Review samples\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,Y_train, X_test, Y_test, classes = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f10f9afccd0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = 11\n",
    "# Implement the code to review the picture with index = 11 and print the labeled class \n",
    "plt.figure()\n",
    "plt.title(f\"y = {Y_train[11]}, it's a '{classes[Y_train[11]]}' picture\")\n",
    "plt.imshow(X_train[11])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "###  Expected result\n",
    "\n",
    "</font>\n",
    "\n",
    "<img src = \"data/cat_11.png\" >\n",
    "\n",
    "y = 1, it's a 'cat' picture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "###  Analyze dimensions   \n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape = (209, 64, 64, 3)\n",
      "X_test.shape = (50, 64, 64, 3)\n",
      "Y_train.shape = (209,)\n",
      "Y_test.shape = (50,)\n",
      "Number of training examples: m_train = 209\n",
      "Number of testing example: m_test = 50\n",
      "Height/Width of each image: num_px = 64\n",
      "Each image is of size: (64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "# Implement the code to print the outpus provided in expected result\n",
    "print(f\"X_train.shape = {X_train.shape}\")\n",
    "print(f\"X_test.shape = {X_test.shape}\")\n",
    "\n",
    "print(f\"Y_train.shape = {Y_train.shape}\")\n",
    "print(f\"Y_test.shape = {Y_test.shape}\")\n",
    "\n",
    "print(f\"Number of training examples: m_train = {X_train.shape[0]}\")\n",
    "print(f\"Number of testing example: m_test = {X_test.shape[0]}\")\n",
    "\n",
    "print(f\"Height/Width of each image: num_px = {X_train.shape[1]}\")\n",
    "print(f\"Each image is of size: ({X_train.shape[1]}, {X_train.shape[2]}, 3)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "###  Expected result\n",
    "\n",
    "</font>\n",
    "\n",
    "`X_train.shape=  (209, 64, 64, 3)\n",
    "X_test.shape=  (50, 64, 64, 3)\n",
    "Y_train.shape=  (209,)\n",
    "Y_test.shape=  (50,)\n",
    "Number of training examples: m_train = 209\n",
    "Number of testing examples: m_test = 50\n",
    "Height/Width of each image: num_px = 64\n",
    "Each image is of size: (64, 64, 3)`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "###  Reshape data\n",
    "\n",
    "</font>\n",
    "\n",
    "$X$ - input features of shape = $(n_{x}, m)$ <br> \n",
    "$Y$ - labels of shape = $(1, m)$ <br> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_flatten shape: (12288, 209)\n",
      "Y_train shape: (1, 209)\n",
      "X_test_flatten shape: (12288, 50)\n",
      "Y_test shape: (1, 50)\n"
     ]
    }
   ],
   "source": [
    "# Reshape the data \n",
    "X_train_flatten = X_train.reshape(-1, X_train.shape[0])\n",
    "X_test_flatten =  X_test.reshape(-1, X_test.shape[0])\n",
    "\n",
    "Y_train = Y_train.reshape(-1, X_train.shape[0])\n",
    "Y_test = Y_test.reshape(-1, X_test.shape[0])\n",
    "print(f\"X_train_flatten shape: {X_train_flatten.shape}\")\n",
    "print(f\"Y_train shape: {Y_train.shape}\")\n",
    "print(f\"X_test_flatten shape: {X_test_flatten.shape}\")\n",
    "print(f\"Y_test shape: {Y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "###  Expected result\n",
    "\n",
    "</font>\n",
    "\n",
    "`X_train_flatten shape: (12288, 209)\n",
    "Y_train shape: (1, 209)\n",
    "X_test_flatten shape: (12288, 50)\n",
    "Y_test shape: (1, 50)`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "###  Scale data\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = X_train_flatten/255.\n",
    "X_test_scaled = X_test_flatten/255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "    \n",
    "## Initialize parameters\n",
    "\n",
    "</font>\n",
    "<br>Shape of $\\quad W^{[l]} = (n^{[l]}, n^{[l-1]})\\,\\quad$     Shape of $\\quad b^{[l]} = (n^{[l]}, 1)$ \n",
    "\n",
    "Note: Initialization of $W = random  \\,/   \\sqrt{ n^{[l-1]} }  $ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(layer_dims):\n",
    "    \"\"\"\n",
    "    layer_dims - list containing the dimensions of each layer in our network including input layer e.g. [12288,7,1]\n",
    "    Returns: dictionary with keys \"W\" and \"b\" and their values are dicts with keys corresponding to layers numbers.\n",
    "        for 'W' - value for every layer is weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "        for 'b' - bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"    \n",
    "    np.random.seed(1)\n",
    "    parameters = {'W':{}, 'b':{}}\n",
    "\n",
    "    # Implement initialization using np.random.randn to match expected result\n",
    "    \n",
    "    parameters[\"W\"] = [\n",
    "        np.random.randn(\n",
    "            layer_dims[layer], layer_dims[layer - 1]\n",
    "            ) / np.sqrt(layer_dims[layer - 1])\n",
    "        for layer in range(len(layer_dims))]\n",
    "\n",
    "    parameters[\"b\"] = [np.zeros((l, 1)) for l in layer_dims]\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W[1] =\n",
      "[[-0.37347383 -0.75870339]\n",
      " [ 0.6119356  -1.62743362]\n",
      " [ 1.23376823 -0.53825456]]\n",
      "b[1] =\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "\n",
      "W[2] =\n",
      "[[ 0.18419731 -0.14397405  0.84414841]\n",
      " [-1.18942279 -0.18614766 -0.22173389]\n",
      " [ 0.65458209 -0.63502252 -0.09955147]\n",
      " [-0.50683179  0.02437212  0.33648852]\n",
      " [-0.63544278  0.66090654  0.52053365]]\n",
      "b[2] =\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "\n",
      "W[3] =\n",
      "[[ 0.2247223   0.40287503 -0.30577239 -0.05495818 -0.41848881]]\n",
      "b[3] =\n",
      "[[0.]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the initialize_parameters()\n",
    "layer_dims= [2,3,5,1] \n",
    "params = initialize_parameters(layer_dims)\n",
    "for l in range(1,len(layer_dims)):\n",
    "    print ('W[{0}] =\\n{1}\\nb[{0}] =\\n{2}\\n'.format(l, params['W'][l], params['b'][l] ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "###  Expected result\n",
    "\n",
    "</font>\n",
    "\n",
    "`W[1] =\n",
    "[[ 1.14858562 -0.43257711]\n",
    " [-0.37347383 -0.75870339]\n",
    " [ 0.6119356  -1.62743362]]\n",
    "b[1] =\n",
    "[[0.]\n",
    " [0.]\n",
    " [0.]]\n",
    "W[2] =\n",
    "[[ 1.00736754 -0.43948301  0.18419731]\n",
    " [-0.14397405  0.84414841 -1.18942279]\n",
    " [-0.18614766 -0.22173389  0.65458209]\n",
    " [-0.63502252 -0.09955147 -0.50683179]\n",
    " [ 0.02437212  0.33648852 -0.63544278]]\n",
    "b[2] =\n",
    "[[0.]\n",
    " [0.]\n",
    " [0.]\n",
    " [0.]\n",
    " [0.]]\n",
    "W[3] =\n",
    "[[ 0.51193601  0.40320363  0.2247223   0.40287503 -0.30577239]]\n",
    "b[3] =\n",
    "[[0.]]`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "    \n",
    "## Forward propagation step\n",
    "\n",
    "</font>\n",
    "\n",
    "$A^{[l]} = g(Z^{[l]}) = g(W^{[l]}A^{[l-1]} +b^{[l]})$, where $g(Z)$ is one of activation functions: \n",
    "\n",
    "$\\sigma(Z) = \\frac{1}{ 1 + e^{-Z}}, \\quad\\quad\\quad RELU(Z) = max(0, Z)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation_step(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    A_prev - activations from previous layer: (size of previous layer, number of examples)\n",
    "    W - weights matrix: array of shape (size of current layer, size of previous layer)\n",
    "    b - bias vector, array of shape (size of the current layer, 1)\n",
    "    activation - text string \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    A -  post-activation value \n",
    "    cache - tuple containing W, b, A_prev, Z stored for computing the backward pass\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = W @ A_prev + b\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        A = sigmoid(Z)\n",
    "        \n",
    "    elif activation == \"relu\":\n",
    "        A = relu(Z)\n",
    "\n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "\n",
    "    cache = (W, b, A_prev, Z) # used at backward propagation. Note: b looks as need just to check the shape of dJ_db\n",
    "    return A, cache\n",
    "\n",
    "\n",
    "def sigmoid(Z):\n",
    "    return 1/(1+np.exp(-Z))\n",
    "\n",
    "def relu(Z):\n",
    "    return np.maximum(0,Z)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "    \n",
    "## Compute cost\n",
    "\n",
    "</font>\n",
    "$$ \\mathcal{L} = -\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right))$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(A_last, Y):\n",
    "    \"\"\"\n",
    "    A_last - vector of predicted probabilties - activations of last layer L, shape (1, number of examples)\n",
    "    Y - true label e.g. cat vs non-cat, shape (1, number of examples)\n",
    "    Returns:\n",
    "    cost - cross-entropy cost\n",
    "    \"\"\"\n",
    "\n",
    "    assert (A_last.shape == Y.shape)\n",
    "    cost = -1 / len(Y) * sum(\n",
    "            [\n",
    "                (y * np.log(a) + (1 - y) * np.log(1 - a))\n",
    "                for y, a in zip(Y, A_last)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    assert(cost.shape == ()) \n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "    \n",
    "## Init backward propagation \n",
    "\n",
    "</font>\n",
    "\n",
    "Compute the derivative for last layer: \n",
    "$$\\frac{\\partial \\mathcal{L}}{dA^{[L]}} =  -\\frac{Y}{A^{[L]}} + \\frac{1 - Y}{1 - A^{[L]}}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_backward_propagation(Y, A_last):\n",
    "    dL_dA_last =  -Y/A + ((1 - Y) / (1 - A))\n",
    "    return dL_dA_last"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "    \n",
    "## Backward propagation step\n",
    "\n",
    "</font>\n",
    "\n",
    "$$ \\frac{\\partial \\mathcal{L} }{\\partial Z^{[l]}} = \\frac{\\partial \\mathcal{L} }{\\partial A^{[l]}} \\cdot \n",
    "\\frac{\\partial g^{[L]}}{\\partial z}(Z^{[L]}); \\quad \n",
    "\\frac { d\\sigma }{ dz } = \\sigma(z)(1-\\sigma(z));\n",
    "\\quad  \n",
    "\\frac { d}{dz}(RELU) = \\begin{cases} 0, z \\le 0 \\\\ 1,\\quad z > 0\\quad \\end{cases}\n",
    "\\\\ \\quad \\\\\n",
    "\\frac{\\partial \\mathcal{L} }{\\partial W^{[l]}} = \\frac{1}{m} \\frac{\\partial \\mathcal{L} }{\\partial Z^{[l]}} \\,@\\, A^{[l-1] T} \\quad \\quad  \n",
    "\\frac{\\partial \\mathcal{L} }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} \\frac{\\partial \\mathcal{L} }{\\partial Z^{[l]}} \\quad (axis=1)\\quad \\quad  \n",
    "\\frac{\\partial \\mathcal{L} }{\\partial A^{[l-1]}} = W^{[l] T} \\,@\\, \\frac{\\partial \\mathcal{L} }{\\partial Z^{[l]}}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation_step(dL_dA, cache, activation):\n",
    "    \"\"\"\n",
    "    dL_dA - activation gradient for current layer l\n",
    "    cache - (W, b, A_prev, Z) stored for current layer  l\n",
    "    activation - string: \"sigmoid\" or \"relu\"\n",
    "    \n",
    "    Returns:\n",
    "    dL_dA_prev - Gradient activation of the previous layer l-1, same shape as A_prev\n",
    "    dL_dW - Gradient of W current layer l, same shape as W\n",
    "    dL_db - Gradient of b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    W, b, A_prev, Z = cache \n",
    "\n",
    "    # backward activation part:\n",
    "    if activation == \"relu\":\n",
    "        dg_dz = relu_backward(Z)\n",
    "    elif activation == \"sigmoid\":\n",
    "        dg_dz = sigmoid_backward(Z)\n",
    "        \n",
    "    assert (dL_dA.shape == dg_dz.shape)\n",
    "    dL_dZ = dl_dA * dg_dz\n",
    "\n",
    "    # backward linear part:\n",
    "    dL_dW = [\n",
    "        1 / len(A_prev) * dL_dZ @ A_prev[l-1].T\n",
    "        for l in range(len(A_prev))\n",
    "        ]\n",
    "        \n",
    "    dL_db = [\n",
    "        1 / len(A_prev) * dL_dZ\n",
    "        for _ in range(len(b))\n",
    "    ]\n",
    "    dL_dA_prev = W.T @ dL_dZ\n",
    "    \n",
    "\n",
    "    assert (dL_dA_prev.shape == A_prev.shape)\n",
    "    assert (dL_dW.shape == W.shape)\n",
    "    assert (dL_db.shape == b.shape)\n",
    "\n",
    "    return dL_dA_prev, dL_dW, dL_db\n",
    "        \n",
    "\n",
    "def relu_backward(Z):\n",
    "    dg_dz = lambda Z: 0 if Z <= 0 else 1\n",
    "    assert (dg_dz.shape == Z.shape)    \n",
    "    return dg_dz\n",
    "\n",
    "\n",
    "\n",
    "def sigmoid_backward(Z):\n",
    "\n",
    "    dg_dz = sigmoid(Z) * (1 - sigmoid(Z))\n",
    "    assert (dg_dz.shape == Z.shape)    \n",
    "    return dg_dz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "    \n",
    "## Update parameters \n",
    "\n",
    "</font>\n",
    "\n",
    "\n",
    "For  $l \\in (1.. L)$:\n",
    "\n",
    "$W^{[l]}  = W^{[l]} - \\alpha \\cdot \\frac{\\partial \\mathcal{L}}{\\partial W^{[l]}} \\quad\\quad\n",
    "b^{[l]}  = b^{[l]} - \\alpha \\cdot \\frac{\\partial \\mathcal{L}}{\\partial b^{[l]}}$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters due to gradient descent rule \n",
    "    parameters - dictionary with keys 'W' and 'b' each is dict with keys of layer numbers \n",
    "    grads - dictionary with keys 'W' and 'b' each is dict with keys of layer numbers \n",
    "   \n",
    "    Returns: updated parameters the same shape as input parameters \n",
    "    \"\"\"\n",
    "   \n",
    "    parameters = None\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "    \n",
    "## Build two-layer model\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):\n",
    "    '''\n",
    "    X - input layer of shape (input size, number of examples)\n",
    "    Y - output layer of shape (1,m)\n",
    "    layers_dims - list of layers dims including input layer \n",
    "    '''\n",
    "  \n",
    "    np.random.seed(1)\n",
    "    grads = {'W':{}, 'b':{}}\n",
    "    costs = []   # track the cost\n",
    "    m = X.shape[1] # number of examples\n",
    "\n",
    "    # Initialize parameters \n",
    "    parameters = None\n",
    "\n",
    "    # Loop (gradient descent)\n",
    "    for i in range(None):\n",
    "\n",
    "        # Forward propagation: LINEAR -> RELU -> LINEAR -> SIGMOID.\n",
    "        A1, cache1 = None\n",
    "        A2, cache2 = None\n",
    "\n",
    "        # Compute cost        \n",
    "        cost = None\n",
    "                \n",
    "        # Initialize backward propagation        \n",
    "        dL_dA2 = None\n",
    "\n",
    "        # Backward propagation.\n",
    "        dL_dA1, grads['W'][2], grads['b'][2] = None\n",
    "        _, grads['W'][1], grads['b'][1] = None\n",
    "        \n",
    "       \n",
    "        # Update parameters    \n",
    "        parameters =  None\n",
    "        \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "       \n",
    "    # plot the cost\n",
    "    None\n",
    "\n",
    "    \n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "## Train the model learning\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_x = X_train_scaled.shape[0]\n",
    "n_h= 7 \n",
    "n_y = Y_train.shape[0]\n",
    "layers_dims = [n_x, n_h, n_y]\n",
    "\n",
    "parameters = two_layer_model(\n",
    "    X_train_scaled, Y_train, layers_dims, learning_rate = 0.003, num_iterations = 3000, print_cost=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "###  Expected result\n",
    "\n",
    "</font>\n",
    "\n",
    "`Cost after iteration 0: 0.6950464961800915\n",
    "Cost after iteration 100: 0.6195808854384666\n",
    "Cost after iteration 200: 0.5865026104533535\n",
    "Cost after iteration 300: 0.5467810398248231\n",
    "Cost after iteration 400: 0.49825722524914073\n",
    "Cost after iteration 500: 0.4565738532427634\n",
    "Cost after iteration 600: 0.4094471539583378\n",
    "Cost after iteration 700: 0.3631730375845946\n",
    "Cost after iteration 800: 0.32861131098831003\n",
    "Cost after iteration 900: 0.29718068617894683\n",
    "Cost after iteration 1000: 0.269008932771751\n",
    "Cost after iteration 1100: 0.24464399315834634\n",
    "Cost after iteration 1200: 0.22384078814076205\n",
    "Cost after iteration 1300: 0.20652180532903278\n",
    "Cost after iteration 1400: 0.1893838186161027\n",
    "Cost after iteration 1500: 0.17501499357058234\n",
    "Cost after iteration 1600: 0.16159871226192452\n",
    "Cost after iteration 1700: 0.14745343426258842\n",
    "Cost after iteration 1800: 0.13478215612014718\n",
    "Cost after iteration 1900: 0.1231337962128783\n",
    "Cost after iteration 2000: 0.11228120642500024\n",
    "Cost after iteration 2100: 0.10296514643048342\n",
    "Cost after iteration 2200: 0.09462444340407362\n",
    "Cost after iteration 2300: 0.08577949788493319\n",
    "Cost after iteration 2400: 0.07838067328265923\n",
    "Cost after iteration 2500: 0.07228557169893012\n",
    "Cost after iteration 2600: 0.06685754016383366\n",
    "Cost after iteration 2700: 0.06210220111202463\n",
    "Cost after iteration 2800: 0.05787619296178021\n",
    "Cost after iteration 2900: 0.053884527381680314`\n",
    "\n",
    "<img src = \"data/19_check_2_layer.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "    \n",
    "### Evaluate 2-layers model\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_two_layers(X, Y, parameters):\n",
    "    \"\"\"        \n",
    "    X - array to predict \n",
    "    parameters - parameters of the trained model\n",
    "    \"\"\"\n",
    "  \n",
    "    # Forward propagation\n",
    "    A1,_ = None\n",
    "    Y_pred, _ = None\n",
    "\n",
    "    accuracy =  None\n",
    "    print(\"Accuracy: {:.3f}\".format( accuracy))\n",
    "       \n",
    "\n",
    "evaluate_two_layers(X_train_scaled, Y_train, parameters)\n",
    "test_accuracy = evaluate_two_layers(X_test_scaled, Y_test, parameters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "    \n",
    "## Forward propagation whole process\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def forward_propagation_whole_process(X, parameters):\n",
    "    \"\"\"\n",
    "    [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID\n",
    "    X - data, array of shape (input size, number of examples)\n",
    "    parameters - initialized parameters foreach of 'W' and 'b' keas values have keys 1,2,...L \n",
    "    \n",
    "    Returns:\n",
    "    A_last - last activation value (y_pred)\n",
    "    caches - dict of caches containing every cache of forward propagation indexed from 0 to L-1\n",
    "    \"\"\"\n",
    "\n",
    "    caches = {}\n",
    "    A = X\n",
    "    \n",
    "    L = len(parameters['W']) # number of layers in the neural network\n",
    "\n",
    "    # [LINEAR -> RELU]*(L-1)\n",
    "    for l in range(1, L):\n",
    "        A_prev = None\n",
    "        A, cache = None\n",
    "        caches[l] = None\n",
    "\n",
    "    \n",
    "    #LINEAR -> SIGMOID\n",
    "    A_last, cache = None\n",
    "    caches[L] = None\n",
    "\n",
    "    assert(A_last.shape == (1, X.shape[1])) # (1,m) \n",
    "            \n",
    "    return A_last, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "    \n",
    "## Backward propagation whole process\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation_whole_process(A_last, Y, caches):\n",
    "    \"\"\"\n",
    "    backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID \n",
    "    A_last - probability vector, output(y_pred) of the forward propagation \n",
    "    Y - true labels (0 if non-cat, 1 if cat)\n",
    "    caches - dict of caches for each layer that contains (W, b, A, Z)\n",
    "    Returns: grads - of keys 'W' and 'b' each containing the  dictionaries of keys 1..L  \n",
    "    \"\"\"\n",
    "    dL_dA= {}\n",
    "    dL_dW = {}\n",
    "    dL_db= {}\n",
    "    \n",
    "    L = len(caches) # the number of layers\n",
    "    m = None # number of samples\n",
    "    Y = Y.reshape(A_last.shape) # make sure Y is the same shape as A_last(y_pred)\n",
    "    \n",
    "    # Initialize the backpropagation    \n",
    "    dL_dA[L] = None\n",
    "\n",
    "    # layer (SIGMOID -> LINEAR) gradients\n",
    "    current_cache = None\n",
    "    dL_dA[L-1], dL_dW[L], dL_db[L] = None\n",
    "\n",
    "    # Loop from l=L-2 to l=0\n",
    "    for l in reversed(range(1,L)): #  starts with L-1 ends with 1 \n",
    "        # l-th layer: (RELU -> LINEAR) gradients.\n",
    "        current_cache = None\n",
    "        dL_dA[l-1], dL_dW[l], dL_db[l] = None\n",
    "        \n",
    "    grads= None\n",
    "    \n",
    "    return grads\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "    \n",
    "## Build deep neural network model \n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, verbose = False):\n",
    "    \"\"\"\n",
    "    X - data, array of shape (number of examples, num_px * num_px * 3)\n",
    "    Y - true label vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims - list containing the input size and each layer size, of length (number of layers + 1).\n",
    "    learning_rate - learning rate of the gradient descent update rule\n",
    "    num_iterations - number of iterations of the optimization loop\n",
    "    verbose - if True, it prints the cost every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    parameters - parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    print ('Training {}-layers neural network with layers dimensions: {}'.format (len(layers_dims)-1, layers_dims))\n",
    "    np.random.seed(1)\n",
    "    costs = [] # to track of cost\n",
    "            \n",
    "    parameters = None\n",
    "        \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(num_iterations):\n",
    "\n",
    "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
    "        A_last, caches = None\n",
    "    \n",
    "        # Compute cost\n",
    "        cost = None\n",
    "\n",
    "        # Backward propagation.\n",
    "        grads = None\n",
    "\n",
    "        # Update parameters.\n",
    "        parameters = None\n",
    "       \n",
    "                \n",
    "        # Print the cost every 100 training example\n",
    "        if verbose and i % 100 == 0:\n",
    "            print (\"Cost after iteration {}: {}\".format(i, cost))\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "    # plot the cost\n",
    "    None\n",
    "    \n",
    "   \n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "    \n",
    "### Run for 2 layers \n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_x = X_train_scaled.shape[0]\n",
    "n_h= 7 \n",
    "n_y = Y_train.shape[0]\n",
    "layers_dims = [n_x, n_h, n_y]\n",
    "\n",
    "parameters = model(\n",
    "    X_train_scaled, Y_train, layers_dims, num_iterations = 5000, verbose = True, \n",
    "    learning_rate = 0.003) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "###  Expected result\n",
    "\n",
    "</font>\n",
    "\n",
    "`Training 2-layers neural network with layers dimensions: [12288, 7, 1]\n",
    "Cost after iteration 0: 0.6950464961800915\n",
    "Cost after iteration 100: 0.6195808854384666\n",
    "Cost after iteration 200: 0.5865026104533535\n",
    "Cost after iteration 300: 0.5467810398248231\n",
    "Cost after iteration 400: 0.49825722524914073\n",
    "...\n",
    "Cost after iteration 4500: 0.023333388131945094\n",
    "Cost after iteration 4600: 0.022437269637948253\n",
    "Cost after iteration 4700: 0.021604560218738137\n",
    "Cost after iteration 4800: 0.02083238901344186\n",
    "Cost after iteration 4900: 0.020094155779688965`\n",
    "\n",
    "<img src = \"data/19_check_2_layer_2.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "    \n",
    "### Run for 4 layers \n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_x = X_train_flatten.shape[0]\n",
    "n_y = Y_train.shape[0]\n",
    "layers_dims = [n_x,20,7,5,n_y]\n",
    "\n",
    "parameters = model(\n",
    "    X_train_scaled, Y_train, layers_dims, num_iterations = 3000, verbose = True, \n",
    "    learning_rate = 0.0075) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "###  Expected result\n",
    "\n",
    "</font>\n",
    "\n",
    "`Training 4-layers neural network with layers dimensions: [12288, 20, 7, 5, 1]\n",
    "Cost after iteration 0: 0.7717493284237688\n",
    "Cost after iteration 100: 0.6720534400822913\n",
    "Cost after iteration 200: 0.6482632048575212\n",
    "Cost after iteration 300: 0.6115068816101354\n",
    "Cost after iteration 400: 0.567047326836611\n",
    "...\n",
    "Cost after iteration 2500: 0.08841251177615041\n",
    "Cost after iteration 2600: 0.08595130416146428\n",
    "Cost after iteration 2700: 0.08168126914926334\n",
    "Cost after iteration 2800: 0.07824661275815534\n",
    "Cost after iteration 2900: 0.07544408693855482`\n",
    "\n",
    "<img src = \"data/19_check_4_layer.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "    \n",
    "## Predict \n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, parameters):\n",
    "    \"\"\"        \n",
    "    X - array set to predict \n",
    "    parameters - parameters of the trained model\n",
    "    Returns:\n",
    "    Y_pred - predictions for the given dataset X\n",
    "    \"\"\"\n",
    "\n",
    "    # Forward propagation\n",
    "    A_last, _ = None\n",
    "    Y_pred= None\n",
    "    \n",
    "    return Y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "    \n",
    "## Evaluate\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Y_pred_train = predict (X_train_scaled, parameters)\n",
    "Y_pred_test = predict (X_test_scaled, parameters)\n",
    "\n",
    "print ('Train accuracy = {:.3%}'.format(np.mean (Y_pred_train == Y_train)))\n",
    "print ('Test accuracy = {:.3%}'.format(np.mean (Y_pred_test == Y_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "###  Expected result\n",
    "\n",
    "</font>\n",
    "\n",
    "`Train accuracy = 99.043%\n",
    "Test accuracy = 82.000%`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "## Learn more\n",
    "\n",
    "</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Neural Networks and Deep Learning](https://www.coursera.org/learn/neural-networks-deep-learning/home/welcome)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "731044aaae828eb69b99b121b388548638cd2a9b9b3e14a53ff293304226a528"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit ('datacamp_home_tasks': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
